{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -P ~/ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! chmod +x ~/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! ~/Miniconda3-latest-Linux-x86_64.sh -b -u\n",
    "# ! export PATH=~/miniconda3/bin:$PATH\n",
    "# ! conda init & conda# ! wget -P ~/ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! conda init & conda config --set auto_activate_base false\n",
    "# # close and start a new session\n",
    "# ! conda activate base\n",
    "# ! conda install cudatoolkit=11.0 -y\n",
    "# ! pip install sentence-transformers   transformers datasets peft accelerate bitsandbytes \n",
    "# ! conda install -c pytorch -c nvidia faiss-gpu=1.8.0\n",
    "# !pip install faiss-gpu #config --set auto_activate_base false\n",
    "# # close and start a new session\n",
    "# ! conda activate base\n",
    "# ! conda install cudatoolkit=11.0 -y\n",
    "# ! pip install sentence-transformers   transformers datasets peft accelerate bitsandbytes faiss-cpu faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, AdamW\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 101093\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 808731\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 101092\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ms_marco', 'v2.1')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c1727692b24d63bf6c2725a53eb262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/808731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153725\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = dataset['train'].select(range(10000))\n",
    "train_dataset = dataset['train']\n",
    "train_dataset = train_dataset.filter(lambda example: example['wellFormedAnswers'] != [] and example['wellFormedAnswers'] != \"\")\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique Documents List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_passages = set()\n",
    "for row in train_dataset:\n",
    "    unique_passages.update(row['passages']['passage_text'])\n",
    "print(len(unique_passages))\n",
    "documents = list(unique_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HUgginface login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "SentenceTranformer = SentenceTransformer(\n",
    "    'sentence-transformers/msmarco-bert-base-dot-v5',\n",
    "    device = device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length:  768\n",
      "Similarity: tensor([[166.5561, 159.5406]])\n"
     ]
    }
   ],
   "source": [
    "query_embedding = SentenceTranformer.encode('How big is London')\n",
    "print(\"embedding length: \", len(query_embedding))\n",
    "document_embedding = SentenceTranformer.encode(\n",
    "    [\n",
    "        'London has 9,787,426 inhabitants at the 2011 census',\n",
    "        'London is known for its finacial district',\n",
    "    ])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, document_embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Embeddings from all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c546912461aa4c78be7833544b409cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Encode documents\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m document_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTranformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py:397\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    396\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 397\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    401\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Encode documents\n",
    "document_embeddings = SentenceTranformer.encode(\n",
    "    documents, \n",
    "    show_progress_bar=True, \n",
    "    device = device,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Faiss Index from all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.normalize_L2(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(document_embeddings.shape[1])  # L2 distance\n",
    "index.add(document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Faiss index to storage and read from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"index_docs.index\")\n",
    "# index = faiss.read_index(\"index_docs.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Faiss Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = faiss.StandardGpuResources() \n",
    "# index_gpu = faiss.index_cpu_to_gpu(res, 0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:  [[0.91807806 0.9110957  0.90587014 0.9055062  0.90503293]\n",
      " [0.8893355  0.8893313  0.8817889  0.8810846  0.8801883 ]]\n",
      "I:  [[13310 10137 12591   653  6796]\n",
      " [ 1994  5586 13499 10405 14510]]\n",
      "tensor([[0.2018, 0.2004, 0.1993, 0.1993, 0.1992],\n",
      "        [0.2010, 0.2010, 0.1995, 0.1993, 0.1992]])\n"
     ]
    }
   ],
   "source": [
    "test_query = [\"This is a query test.\", \"This is a query donut.\"]\n",
    "test_query_embedding = SentenceTranformer.encode(test_query)\n",
    "# print(\"test_query_embedding: \", test_query_embedding)\n",
    "faiss.normalize_L2(test_query_embedding)\n",
    "\n",
    "k = 5  # Number of similar documents to retrieve\n",
    "D, I = index.search(test_query_embedding, k)\n",
    "print(\"D: \", D)\n",
    "print(\"I: \", I)\n",
    "D_tensor = torch.tensor(D)\n",
    "D_softmax = F.softmax(D_tensor, dim=1)  # Apply softmax along the rows\n",
    "D_softmax_np = D_softmax.numpy()\n",
    "print(D_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure LoRA and sentenceTranformer of query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "queryTranformer = SentenceTransformer(\n",
    "    'sentence-transformers/msmarco-bert-base-dot-v5',\n",
    "    device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sentenceTransformer in training mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LoRA and EncoderDecoder GPT2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "# Generator = GPT2Model.from_pretrained('gpt2')\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "generator = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side='left', pad_token = \"<|pad|>\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\",  padding_side='left', pad_token = None)\n",
    "# 50257 - output seq\n",
    "print(generator)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define GPT2 in training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom lost function???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training object combined????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1503, 20])\n",
      "<class 'torch.Tensor'> torch.Size([1503, 20])\n",
      "<class 'torch.Tensor'> torch.Size([1503, 105])\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 768\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(\n",
    "    train_dataset['query'], \n",
    "    max_length=max_seq_len, \n",
    "    padding='longest', \n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "# print(type(inputs))\n",
    "print(type(inputs['input_ids']), inputs['input_ids'].shape)\n",
    "print(type(inputs['attention_mask']), inputs['attention_mask'].shape)\n",
    "\n",
    "\n",
    "flattened_answers = [sublist[0] for sublist in train_dataset['wellFormedAnswers']]\n",
    "# Prepend <sos> token (this will handle the right shit)\n",
    "\n",
    "targets = tokenizer(\n",
    "    flattened_answers, \n",
    "    max_length=max_seq_len, \n",
    "    padding='longest', \n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(type(targets['input_ids']), targets['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataset = TensorDataset(inputs['input_ids'].to(device), inputs['attention_mask'].to(device), targets['input_ids'].to(device), targets['attention_mask'].to(device))\n",
    "trainingDataloader = DataLoader(trainingDataset, batch_size, shuffle=False,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperModel(torch.nn.Module):\n",
    "    def __init__(self, generator, queryTranformer, index, documents):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.queryTranformer = queryTranformer\n",
    "        self.documents = documents\n",
    "        self.index = index\n",
    "\n",
    "    def forward(self, batch_inputs_ids, attention_masks, batch_label_ids, labels_attention_mask, K, max_seq_len):\n",
    "        \n",
    "        queries = tokenizer.batch_decode(\n",
    "            batch_inputs_ids,\n",
    "            skip_special_tokens = True\n",
    "        )\n",
    "\n",
    "        answers = tokenizer.batch_decode(\n",
    "            batch_label_ids,\n",
    "            skip_special_tokens = True\n",
    "        )\n",
    "\n",
    "        queries_embedding = self.queryTranformer.encode(queries)  # Pass appropriate inputs\n",
    "\n",
    "        D, I = index.search(queries_embedding, K)\n",
    "        D_tensor = torch.tensor(D).to(device)\n",
    "        D_softmax = F.softmax(D_tensor, dim=-1) \n",
    "\n",
    "        prompts = []\n",
    "\n",
    "        for ind in range(len(I)):\n",
    "            for i, idx in enumerate(I[ind]):\n",
    "                prompt = self.documents[idx] + \" \" + queries[ind] + \"? \" + answers[ind] #No look ahead mask wanted here \n",
    "                prompts.append(prompt)\n",
    "\n",
    "\n",
    "        tokinized_prompts_ids = tokenizer(\n",
    "            prompts, \n",
    "            max_length=max_seq_len, \n",
    "            padding='longest', \n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # look ahead mask - one token at a time \n",
    "        \n",
    "        input_ids = tokinized_prompts_ids[\"input_ids\"].to(device).view(len(queries), K, -1)\n",
    "        attention_mask = tokinized_prompts_ids[\"attention_mask\"].to(device).view(len(queries), K, -1)\n",
    "\n",
    "        generator.resize_token_embeddings(len(tokenizer))\n",
    "        # input_ids = \n",
    "        generatorOutput = self.generator(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "\n",
    "        softmaxed_logits = F.softmax(generatorOutput.logits, dim=-1) \n",
    "        D_softmax_unsqueezed = D_softmax.view(D_softmax.size(0), D_softmax.size(1), 1, 1)\n",
    "        weighted_logits = softmaxed_logits * D_softmax_unsqueezed\n",
    "        marginalised = torch.mean(weighted_logits, dim=1)\n",
    "        \n",
    "        shifted_tokinesed_input = None\n",
    "        return marginalised,  attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperModel(\n",
       "  (generator): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       "  (queryTranformer): SentenceTransformer(\n",
       "    (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "    (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SuperModel(\n",
    "    generator = generator , \n",
    "    queryTranformer = queryTranformer , \n",
    "    index = index , \n",
    "    documents = documents ,\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m labels_attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m batch_label_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m logits, attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_inputs_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_label_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels_attention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mK\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# attention_mask\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(\"batch_label_ids: \", batch_label_ids.shape)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Pad at_mask tensor to match SEQ_LEN\u001b[39;00m\n\u001b[1;32m     31\u001b[0m pad_width \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m batch_label_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[141], line 31\u001b[0m, in \u001b[0;36mSuperModel.forward\u001b[0;34m(self, batch_inputs_ids, attention_masks, batch_label_ids, labels_attention_mask, K, max_seq_len)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(I)):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(I[ind]):\n\u001b[0;32m---> 31\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m queries[ind] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m? \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m answers[ind] \u001b[38;5;66;03m#No look ahead mask wanted here \u001b[39;00m\n\u001b[1;32m     32\u001b[0m         prompts\u001b[38;5;241m.\u001b[39mappend(prompt)\n\u001b[1;32m     35\u001b[0m tokinized_prompts_ids \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     36\u001b[0m     prompts, \n\u001b[1;32m     37\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_seq_len, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m )\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for batch in trainingDataloader:  # Size: Batch_size x Seq_length\n",
    "        optimizer.zero_grad()\n",
    "        i+=1\n",
    "\n",
    "        labels_attention_mask = batch[3].to(device)\n",
    "        batch_label_ids = batch[2].to(device)\n",
    "        \n",
    "        logits, attention_mask = model(\n",
    "            batch_inputs_ids = batch[0].to(device), \n",
    "            attention_masks= batch[1].to(device), \n",
    "            batch_label_ids = batch[2].to(device),\n",
    "            labels_attention_mask = batch[3].to(device),\n",
    "            K = K,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "\n",
    "        # attention_mask\n",
    "\n",
    "        # print(\"batch_label_ids: \", batch_label_ids.shape)\n",
    "        # # print(\"batch_label_ids: \", batch_label_ids)\n",
    "        # print(\"attention_mask: \", attention_mask.shape)\n",
    "        # print(\"logits: \", logits.shape)\n",
    "        # print(\"logits: \", logits.size(0), logits.size(1), logits.size(2))\n",
    "\n",
    "        # Pad at_mask tensor to match SEQ_LEN\n",
    "        pad_width = logits.size(1) - batch_label_ids.size(1)\n",
    "        lables_ids_padded = torch.cat((torch.zeros((logits.size(0), pad_width), dtype=torch.long, device = device), batch_label_ids), dim=1)\n",
    "        # print(\"lables_ids_padded: \", lables_ids_padded)\n",
    "        # Fill the padded region with 5437\n",
    "        lables_ids_padded[:, :pad_width].fill_(50256)\n",
    "        # print(\"lables_ids_padded: \", lables_ids_padded)\n",
    "        batch, tokens, vocabs = logits.shape\n",
    "        logits  = logits.view(batch * tokens, vocabs) # Size: [(Batch Size * Tokens Amount) x Vocab Size]\n",
    "        targets = lables_ids_padded.view(batch * tokens)        # Size: [(Batch Size * Tokens Amount)]\n",
    "\n",
    "        # print(targets)\n",
    "        \n",
    "        # Assuming outputs from the model and labels are already obtained\n",
    "        mask = targets != 50256  # Assuming -1 is used for padding in labels\n",
    "\n",
    "        # print(mask)\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "        loss = loss * mask.view(batch * tokens).float()\n",
    "        loss = loss.sum() / mask.sum()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none = True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 2 == 0:  # Print loss every 100 batches\n",
    "            print(f'[{epoch + 1}, {i}] Loss: {loss.item() :.3f} Rloss: {running_loss / 2:.3f}')\n",
    "            running_loss = 0.0\n",
    "        # break;\n",
    "\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sizes\n",
    "B = 3\n",
    "SEQ_LEN = 10\n",
    "SEQ_LEN2 = 5\n",
    "\n",
    "# Example attention_mask and at_mask tensors\n",
    "attention_mask = torch.ones((B, SEQ_LEN))\n",
    "at_mask = torch.ones((B, SEQ_LEN2))\n",
    "\n",
    "# Pad at_mask tensor to match SEQ_LEN\n",
    "pad_width = SEQ_LEN - SEQ_LEN2\n",
    "at_mask_padded = torch.cat((torch.zeros((B, pad_width)), at_mask), dim=1)\n",
    "\n",
    "print(\"at_mask_padded:\")\n",
    "print(at_mask_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.ones([1, 2, 3, 4])\n",
    "print(\"logits: \\n\",logits)\n",
    "\n",
    "softmaxed_logits = F.softmax(logits, dim=-1) \n",
    "\n",
    "print(\"logits soft max: \\n\",softmaxed_logits)\n",
    "\n",
    "probability = custom_tensor = torch.tensor([[0.5, 2]])\n",
    "print(logits.shape, probability.shape)\n",
    "\n",
    "tensor2_expanded = probability.view(probability.size(0), probability.size(1), 1, 1)\n",
    "test = softmaxed_logits * tensor2_expanded\n",
    "\n",
    "print(test.shape)\n",
    "print(test)\n",
    "\n",
    "mean_tensor = torch.mean(test, dim=1)\n",
    "print(mean_tensor.shape)\n",
    "print(mean_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tensor\n",
    "tensor = torch.tensor([\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [0, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "# Compute the sum along the rows (dimension 1)\n",
    "row_sums = torch.sum(tensor, dim=-1)\n",
    "\n",
    "print(row_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = CustomModel(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         loss = custom_loss(logits, labels)\n",
    "#         return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your model, tokenizer, and training arguments\n",
    "# model = ...  # Define your model here\n",
    "# tokenizer = ...  # Define your tokenizer here\n",
    "# training_args = TrainingArguments(\n",
    "#     ...\n",
    "# )  # Define your training arguments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = CustomTrainer(\n",
    "#     modelok=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pack to Docker Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select and Prepare a Pre-trained Seq2Seq Model\n",
    "# Generate the Response\n",
    "# Evaluation and Iteration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
