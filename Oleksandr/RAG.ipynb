{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -P ~/ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! chmod +x ~/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! ~/Miniconda3-latest-Linux-x86_64.sh -b\n",
    "# ! export PATH=~/miniconda3/bin:$PATH\n",
    "# ! conda init & conda# ! wget -P ~/ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! chmod +x ~/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! ~/Miniconda3-latest-Linux-x86_64.sh -b\n",
    "# ! export PATH=~/miniconda3/bin:$PATH\n",
    "# ! conda init & conda config --set auto_activate_base false\n",
    "# # close and start a new session\n",
    "# ! conda activate base\n",
    "# ! conda install cudatoolkit=11.0 -y\n",
    "# ! pip install sentence-transformers   transformers datasets peft accelerate bitsandbytes \n",
    "# ! conda install -c pytorch -c nvidia faiss-gpu=1.8.0\n",
    "# !pip install faiss-gpu config --set auto_activate_base false\n",
    "# # close and start a new session\n",
    "# ! conda activate base\n",
    "# ! conda install cudatoolkit=11.0 -y\n",
    "# ! pip install sentence-transformers   transformers datasets peft accelerate bitsandbytes faiss-cpu faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, AdamW\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 101093\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 808731\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 101092\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ms_marco', 'v2.1')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].select(range(1000))\n",
    "train_dataset = train_dataset.filter(lambda example: example['wellFormedAnswers'] != [] and example['wellFormedAnswers'] != \"\")\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique Documents List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1333\n"
     ]
    }
   ],
   "source": [
    "unique_passages = set()\n",
    "for row in train_dataset:\n",
    "    unique_passages.update(row['passages']['passage_text'])\n",
    "print(len(unique_passages))\n",
    "documents = list(unique_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HUgginface login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "SentenceTranformer = SentenceTransformer(\n",
    "    'sentence-transformers/msmarco-bert-base-dot-v5',\n",
    "    device = device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length:  768\n",
      "Similarity: tensor([[166.5561, 159.5406]])\n"
     ]
    }
   ],
   "source": [
    "query_embedding = SentenceTranformer.encode('How big is London')\n",
    "print(\"embedding length: \", len(query_embedding))\n",
    "document_embedding = SentenceTranformer.encode(\n",
    "    [\n",
    "        'London has 9,787,426 inhabitants at the 2011 census',\n",
    "        'London is known for its finacial district',\n",
    "    ])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, document_embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Embeddings from all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6db632417a431b948afbe09134d639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode documents\n",
    "document_embeddings = SentenceTranformer.encode(\n",
    "    documents, \n",
    "    show_progress_bar=True, \n",
    "    device = device,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Faiss Index from all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.normalize_L2(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(document_embeddings.shape[1])  # L2 distance\n",
    "index.add(document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Faiss index to storage and read from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"index_docs.index\")\n",
    "# index = faiss.read_index(\"index_docs.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Faiss Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = faiss.StandardGpuResources() \n",
    "# index_gpu = faiss.index_cpu_to_gpu(res, 0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:  [[0.90023875 0.89977723 0.89780843 0.8967315  0.8966974 ]\n",
      " [0.8674655  0.867303   0.86647964 0.8658711  0.8655468 ]]\n",
      "I:  [[1144  599  317  818  563]\n",
      " [ 282 1240  935  592  299]]\n",
      "tensor([[0.2004, 0.2003, 0.1999, 0.1997, 0.1997],\n",
      "        [0.2002, 0.2002, 0.2000, 0.1999, 0.1998]])\n"
     ]
    }
   ],
   "source": [
    "test_query = [\"This is a query test.\", \"This is a query donut.\"]\n",
    "test_query_embedding = SentenceTranformer.encode(test_query)\n",
    "# print(\"test_query_embedding: \", test_query_embedding)\n",
    "faiss.normalize_L2(test_query_embedding)\n",
    "\n",
    "k = 5  # Number of similar documents to retrieve\n",
    "D, I = index.search(test_query_embedding, k)\n",
    "print(\"D: \", D)\n",
    "print(\"I: \", I)\n",
    "D_tensor = torch.tensor(D)\n",
    "D_softmax = F.softmax(D_tensor, dim=1)  # Apply softmax along the rows\n",
    "D_softmax_np = D_softmax.numpy()\n",
    "print(D_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure LoRA and sentenceTranformer of query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "queryTranformer = SentenceTransformer(\n",
    "    'sentence-transformers/msmarco-bert-base-dot-v5',\n",
    "    device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sentenceTransformer in training mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LoRA and EncoderDecoder GPT2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "# Generator = GPT2Model.from_pretrained('gpt2')\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "generator = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side='left', pad_token = \"<|pad|>\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", pad_token = None)\n",
    "# 50257 - output seq\n",
    "print(generator)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define GPT2 in training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom lost function???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training object combined????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([134, 768])\n",
      "<class 'torch.Tensor'> torch.Size([134, 768])\n",
      "<class 'torch.Tensor'> torch.Size([134, 768])\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 768\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(\n",
    "    train_dataset['query'], \n",
    "    max_length=max_seq_len, \n",
    "    padding='max_length', \n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "# print(type(inputs))\n",
    "print(type(inputs['input_ids']), inputs['input_ids'].shape)\n",
    "print(type(inputs['attention_mask']), inputs['attention_mask'].shape)\n",
    "\n",
    "\n",
    "flattened_answers = [sublist[0] for sublist in train_dataset['wellFormedAnswers']]\n",
    "# Prepend <sos> token (this will handle the right shit)\n",
    "\n",
    "targets = tokenizer(\n",
    "    flattened_answers, \n",
    "    max_length=max_seq_len, \n",
    "    padding='max_length', \n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(type(targets['input_ids']), targets['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], targets['input_ids'], targets['attention_mask'])\n",
    "trainingDataloader = DataLoader(trainingDataset, batch_size, shuffle=False,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(logits, labels):\n",
    "    loss = torch.mean((logits - labels) ** 2)  # For example, mean squared error\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperModel(torch.nn.Module):\n",
    "    def __init__(self, generator, queryTranformer, index, documents):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.queryTranformer = queryTranformer\n",
    "        self.documents = documents\n",
    "        self.index = index\n",
    "\n",
    "    def forward(self, batch_inputs_ids, attention_masks, batch_label_ids, labels_attention_mask, K, max_seq_len):\n",
    "        \n",
    "        queries = tokenizer.batch_decode(\n",
    "            batch_inputs_ids,\n",
    "            skip_special_tokens = True\n",
    "        )\n",
    "\n",
    "        answers = tokenizer.batch_decode(\n",
    "            batch_label_ids,\n",
    "            skip_special_tokens = True\n",
    "        )\n",
    "\n",
    "        queries_embedding = self.queryTranformer.encode(queries)  # Pass appropriate inputs\n",
    "\n",
    "        D, I = index.search(queries_embedding, K)\n",
    "        D_tensor = torch.tensor(D)\n",
    "        D_softmax = F.softmax(D_tensor, dim=-1) \n",
    "\n",
    "        prompts = []\n",
    "\n",
    "        for ind in range(len(I)):\n",
    "            for i, idx in enumerate(I[ind]):\n",
    "                prompt = self.documents[idx] + \" \" + queries[ind] + \" \" + answers[ind] #No look ahead mask wanted here \n",
    "                prompts.append(prompt)\n",
    "\n",
    "        tokinized_prompts_ids = tokenizer(\n",
    "            prompts, \n",
    "            max_length=max_seq_len, \n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # look ahead mask - one token at a time \n",
    "        \n",
    "        input_ids = tokinized_prompts_ids[\"input_ids\"].view(len(queries), K, -1)\n",
    "        attention_mask = tokinized_prompts_ids[\"attention_mask\"].view(len(queries), K, -1)\n",
    "\n",
    "        generator.resize_token_embeddings(len(tokenizer))\n",
    "        # input_ids = \n",
    "        generatorOutput = self.generator(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "\n",
    "        softmaxed_logits = F.softmax(generatorOutput.logits, dim=-1) \n",
    "        D_softmax_unsqueezed = D_softmax.view(D_softmax.size(0), D_softmax.size(1), 1, 1)\n",
    "        weighted_logits = softmaxed_logits * D_softmax_unsqueezed\n",
    "        marginalised = torch.mean(weighted_logits, dim=1)\n",
    "        \n",
    "        shifted_tokinesed_input = None\n",
    "        return marginalised, shifted_tokinesed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SuperModel(\n",
    "    generator = generator , \n",
    "    queryTranformer = queryTranformer , \n",
    "    index = index , \n",
    "    documents = documents ,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for batch in trainingDataloader:  # Size: Batch_size x Seq_length\n",
    "    optimizer.zero_grad()\n",
    "    i+=1\n",
    "    \n",
    "    outputs = model(\n",
    "        batch_inputs_ids = batch[0], \n",
    "        attention_masks= batch[1], \n",
    "        batch_label_ids = batch[2],\n",
    "        labels_attention_mask = batch[3],\n",
    "        K = K,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "\n",
    "    break\n",
    "    \n",
    "    # loss = outputs.loss\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    # print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.ones([1, 2, 3, 4])\n",
    "print(\"logits: \\n\",logits)\n",
    "\n",
    "softmaxed_logits = F.softmax(logits, dim=-1) \n",
    "\n",
    "print(\"logits soft max: \\n\",softmaxed_logits)\n",
    "\n",
    "probability = custom_tensor = torch.tensor([[0.5, 2]])\n",
    "print(logits.shape, probability.shape)\n",
    "\n",
    "tensor2_expanded = probability.view(probability.size(0), probability.size(1), 1, 1)\n",
    "test = softmaxed_logits * tensor2_expanded\n",
    "\n",
    "print(test.shape)\n",
    "print(test)\n",
    "\n",
    "mean_tensor = torch.mean(test, dim=1)\n",
    "print(mean_tensor.shape)\n",
    "print(mean_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = CustomModel(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         loss = custom_loss(logits, labels)\n",
    "#         return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your model, tokenizer, and training arguments\n",
    "# model = ...  # Define your model here\n",
    "# tokenizer = ...  # Define your tokenizer here\n",
    "# training_args = TrainingArguments(\n",
    "#     ...\n",
    "# )  # Define your training arguments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = CustomTrainer(\n",
    "#     modelok=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pack to Docker Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select and Prepare a Pre-trained Seq2Seq Model\n",
    "# Generate the Response\n",
    "# Evaluation and Iteration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
