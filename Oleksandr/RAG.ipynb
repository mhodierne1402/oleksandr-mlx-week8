{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -P ~/ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! chmod +x ~/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! ~/Miniconda3-latest-Linux-x86_64.sh -b\n",
    "# ! export PATH=~/miniconda3/bin:$PATH\n",
    "# ! conda init & conda# ! wget -P ~/ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! chmod +x ~/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! ~/Miniconda3-latest-Linux-x86_64.sh -b\n",
    "# ! export PATH=~/miniconda3/bin:$PATH\n",
    "# ! conda init & conda config --set auto_activate_base false\n",
    "# # close and start a new session\n",
    "# ! conda activate base\n",
    "# ! conda install cudatoolkit=11.0 -y\n",
    "# ! pip install sentence-transformers   transformers datasets peft accelerate bitsandbytes \n",
    "# ! conda install -c pytorch -c nvidia faiss-gpu=1.8.0\n",
    "# !pip install faiss-gpu config --set auto_activate_base false\n",
    "# # close and start a new session\n",
    "# ! conda activate base\n",
    "# ! conda install cudatoolkit=11.0 -y\n",
    "# ! pip install sentence-transformers   transformers datasets peft accelerate bitsandbytes faiss-cpu faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, AdamW\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 101093\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 808731\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 101092\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ms_marco', 'v2.1')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].select(range(1000))\n",
    "train_dataset = train_dataset.filter(lambda example: example['wellFormedAnswers'] != [] and example['wellFormedAnswers'] != \"\")\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique Documents List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1333\n"
     ]
    }
   ],
   "source": [
    "unique_passages = set()\n",
    "for row in train_dataset:\n",
    "    unique_passages.update(row['passages']['passage_text'])\n",
    "print(len(unique_passages))\n",
    "documents = list(unique_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HUgginface login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "SentenceTranformer = SentenceTransformer(\n",
    "    'sentence-transformers/msmarco-bert-base-dot-v5',\n",
    "    device = device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length:  768\n",
      "Similarity: tensor([[166.5561, 159.5406]])\n"
     ]
    }
   ],
   "source": [
    "query_embedding = SentenceTranformer.encode('How big is London')\n",
    "print(\"embedding length: \", len(query_embedding))\n",
    "document_embedding = SentenceTranformer.encode(\n",
    "    [\n",
    "        'London has 9,787,426 inhabitants at the 2011 census',\n",
    "        'London is known for its finacial district',\n",
    "    ])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, document_embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Embeddings from all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcf094f4d75457badd22d0b24eeaac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode documents\n",
    "document_embeddings = SentenceTranformer.encode(\n",
    "    documents, \n",
    "    show_progress_bar=True, \n",
    "    device = device,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Faiss Index from all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.normalize_L2(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(document_embeddings.shape[1])  # L2 distance\n",
    "index.add(document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Faiss index to storage and read from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"index_docs.index\")\n",
    "# index = faiss.read_index(\"index_docs.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Faiss Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = faiss.StandardGpuResources() \n",
    "# index_gpu = faiss.index_cpu_to_gpu(res, 0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:  [[0.90023875 0.89977723 0.89780843 0.8967315  0.8966974 ]\n",
      " [0.8674655  0.867303   0.86647964 0.8658711  0.8655468 ]]\n",
      "I:  [[ 299  556  759 1116  977]\n",
      " [1223  699  101  792  393]]\n",
      "tensor([[0.2004, 0.2003, 0.1999, 0.1997, 0.1997],\n",
      "        [0.2002, 0.2002, 0.2000, 0.1999, 0.1998]])\n",
      "Most similar documents to the query:\n",
      "Rank 1: A vitamin B12 test measures the amount of vitamin B12 in the blood. The normal values listed here-called a reference range-are just a guide. These ranges vary from lab to lab, and your lab may have a different range for what's normal.\n",
      "Rank 2: Results. A vitamin B12 test measures the amount of vitamin B12 in the blood. The normal values listed here-called a reference range-are just a guide. These ranges vary from lab to lab, and your lab may have a different range for what's normal.\n",
      "Rank 3: It could be argued that, in a powerlifting context, the deadlift is a true measure of strength due to its lack of emphasis on various performance aids (suits etc). It also employs more muscle groups, and therefore could be deemed a better test of overall muscle strength.\n",
      "Rank 4: A vitamin B12 test measures the amount of vitamin B12 in the blood. The normal values listed here-called a reference range-are just a guide. These ranges vary from lab to lab, and your lab may have a different range for what's normal. Your lab report should contain the range your lab uses.\n",
      "Rank 5: Results. A vitamin B12 test measures the amount of vitamin B12 in the blood. The normal values listed here-called a reference range-are just a guide. These ranges vary from lab to lab, and your lab may have a different range for what's normal. Your lab report should contain the range your lab uses.\n",
      "Rank 1: In cuisine, an omelette or omelet is a dish made from beaten eggs quickly fried with butter or oil in a frying pan (without stirring as in scrambled egg). It is quite common for the omelette to be folded around a filling such as cheese, chives, vegetables, meat (often ham or bacon), or some combination of the above. Whole eggs or sometimes only egg whites are beaten with a small amount of milk or cream, or even water.\n",
      "Rank 2: et al. n. abbreviation for the Latin phrase et alii meaning and others.. This is commonly used in shortening the name of a case, as in Pat Murgatroyd v. Sally Sherman, et al.. et al. adverb and all, and everyone, and more of the same, and other parties, and other things, and others, and the rest.\n",
      "Rank 3: We apologize, the Color Compass® Quiz is currently under construction for this mobile device. In the meantime, visit our site on your desktop or laptop to use Color Compass® Quiz. If you’d like us to notify you when Color Compass® Quiz is available please enter your email address.*.\n",
      "Rank 4: Noun[edit] quoll ‎(plural quolls) Any of the various carnivorous marsupials of the genus Dasyurus found in Australia and New Guinea, roughly the size of a cat.\n",
      "Rank 5: Canadian Postal Code Database. Get all Canadian Postal Codes and their information in one easy to use database. 2010 Census Database. Get the 2010 Census data in an easy to use format for all summary levels: National, State, COunty, City, and Congressional District.\n"
     ]
    }
   ],
   "source": [
    "test_query = [\"This is a query test.\", \"This is a query donut.\"]\n",
    "test_query_embedding = SentenceTranformer.encode(test_query)\n",
    "# print(\"test_query_embedding: \", test_query_embedding)\n",
    "faiss.normalize_L2(test_query_embedding)\n",
    "\n",
    "k = 5  # Number of similar documents to retrieve\n",
    "D, I = index.search(test_query_embedding, k)\n",
    "print(\"D: \", D)\n",
    "print(\"I: \", I)\n",
    "D_tensor = torch.tensor(D)\n",
    "D_softmax = F.softmax(D_tensor, dim=1)  # Apply softmax along the rows\n",
    "D_softmax_np = D_softmax.numpy()\n",
    "print(D_softmax)\n",
    "print(\"Most similar documents to the query:\")\n",
    "for i, idx in enumerate(I[0]):\n",
    "    print(f\"Rank {i+1}: {documents[idx]}\")\n",
    "for i, idx in enumerate(I[1]):\n",
    "    print(f\"Rank {i+1}: {documents[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure LoRA and sentenceTranformer of query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "queryTranformer = SentenceTransformer(\n",
    "    'sentence-transformers/msmarco-bert-base-dot-v5',\n",
    "    device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sentenceTransformer in training mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LoRA and EncoderDecoder GPT2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44efc122f22e411d8a6abb7a567dca28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563a42d14da14366aa6cca05969183c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0066747faae41bd93cb9d8c59933dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761a528abe2d4293b1a7bab46d362ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1f2e34c77f4ea9923196ed3f88ef6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6da3b346af94d11837181de66e6b046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a60847817834569af0a94fd0da7b857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|pad|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50257: AddedToken(\"<|pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "# Generator = GPT2Model.from_pretrained('gpt2')\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "generator = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side='left', pad_token = \"<|pad|>\")\n",
    "# 50257 - output seq\n",
    "print(generator)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define GPT2 in training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom lost function???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training object combined????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([134, 768])\n",
      "<class 'torch.Tensor'> torch.Size([134, 768])\n",
      "<class 'torch.Tensor'> torch.Size([134, 768])\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "# print(train_dataset['query'])\n",
    "max_seq_len = 768\n",
    "\n",
    "inputs = tokenizer(\n",
    "    train_dataset['query'], \n",
    "    max_length=max_seq_len, \n",
    "    padding='max_length', \n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "# print(type(inputs))\n",
    "print(type(inputs['input_ids']), inputs['input_ids'].shape)\n",
    "print(type(inputs['attention_mask']), inputs['attention_mask'].shape)\n",
    "\n",
    "\n",
    "flattened_answers = [sublist[0] for sublist in train_dataset['wellFormedAnswers']]\n",
    "# Prepend <sos> token (this will handle the right shit)\n",
    "\n",
    "targets = tokenizer(\n",
    "    flattened_answers, \n",
    "    max_length=max_seq_len, \n",
    "    padding='max_length', \n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(type(targets['input_ids']), targets['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], targets['input_ids'], targets['attention_mask'])\n",
    "trainingDataloader = DataLoader(trainingDataset, batch_size, shuffle=False,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(logits, labels):\n",
    "    loss = torch.mean((logits - labels) ** 2)  # For example, mean squared error\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperModel(torch.nn.Module):\n",
    "    def __init__(self, generator, queryTranformer, index, documents):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.queryTranformer = queryTranformer\n",
    "        self.documents = documents\n",
    "        self.index = index\n",
    "\n",
    "    def forward(self, batch_inputs_ids, attention_masks, label_ids, labels_attention_mask, K, max_seq_len):\n",
    "        \n",
    "        queries = tokenizer.batch_decode(\n",
    "            batch_inputs_ids,\n",
    "            skip_special_tokens = True\n",
    "        )\n",
    "\n",
    "        queries_embedding = self.queryTranformer.encode(queries)  # Pass appropriate inputs\n",
    "\n",
    "        D, I = index.search(queries_embedding, K)\n",
    "        D_tensor = torch.tensor(D)\n",
    "        D_softmax = F.softmax(D_tensor, dim=-1) \n",
    "\n",
    "        prompts = []\n",
    "\n",
    "        for ind in range(len(I)):\n",
    "            for i, idx in enumerate(I[ind]):\n",
    "                prompt = self.documents[idx] + \" \" + queries[ind] #No look ahead mask wanted here \n",
    "                prompts.append(prompt)\n",
    "\n",
    "        tokinized_prompts_ids = tokenizer(\n",
    "            prompts, \n",
    "            max_length=max_seq_len, \n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # look ahead mask - one token at a time \n",
    "        \n",
    "        input_ids = tokinized_prompts_ids[\"input_ids\"].view(len(queries), K, -1)\n",
    "        attention_mask = tokinized_prompts_ids[\"attention_mask\"].view(len(queries), K, -1)\n",
    "\n",
    "        generator.resize_token_embeddings(len(tokenizer))\n",
    "        # input_ids = \n",
    "        generatorOutput = self.generator(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "\n",
    "        softmaxed_logits = F.softmax(generatorOutput.logits, dim=-1) \n",
    "        D_softmax_unsqueezed = D_softmax.view(D_softmax.size(0), D_softmax.size(1), 1, 1)\n",
    "        weighted_logits = softmaxed_logits * D_softmax_unsqueezed\n",
    "        marginalised = torch.mean(weighted_logits, dim=1)\n",
    "        \n",
    "        shifted_tokinesed_input = None\n",
    "        return marginalised, shifted_tokinesed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SuperModel(\n",
    "    generator = generator , \n",
    "    queryTranformer = queryTranformer , \n",
    "    index = index , \n",
    "    documents = documents ,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768, 50258])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in trainingDataloader:  # Size: Batch_size x Seq_length\n",
    "    optimizer.zero_grad()\n",
    "    i+=1\n",
    "    \n",
    "    outputs = model(\n",
    "        batch_inputs_ids = batch[0], \n",
    "        attention_masks= batch[1], \n",
    "        label_ids = batch[2],\n",
    "        labels_attention_mask = batch[3],\n",
    "        K = K,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "\n",
    "    break\n",
    "    \n",
    "    # loss = outputs.loss\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    # print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: \n",
      " tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]]])\n",
      "logits soft max: \n",
      " tensor([[[[0.2500, 0.2500, 0.2500, 0.2500],\n",
      "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "          [0.2500, 0.2500, 0.2500, 0.2500]],\n",
      "\n",
      "         [[0.2500, 0.2500, 0.2500, 0.2500],\n",
      "          [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "          [0.2500, 0.2500, 0.2500, 0.2500]]]])\n",
      "torch.Size([1, 2, 3, 4]) torch.Size([1, 2])\n",
      "torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[0.1250, 0.1250, 0.1250, 0.1250],\n",
      "          [0.1250, 0.1250, 0.1250, 0.1250],\n",
      "          [0.1250, 0.1250, 0.1250, 0.1250]],\n",
      "\n",
      "         [[0.5000, 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000]]]])\n",
      "torch.Size([1, 3, 4])\n",
      "tensor([[[0.3125, 0.3125, 0.3125, 0.3125],\n",
      "         [0.3125, 0.3125, 0.3125, 0.3125],\n",
      "         [0.3125, 0.3125, 0.3125, 0.3125]]])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.ones([1, 2, 3, 4])\n",
    "print(\"logits: \\n\",logits)\n",
    "\n",
    "softmaxed_logits = F.softmax(logits, dim=-1) \n",
    "\n",
    "print(\"logits soft max: \\n\",softmaxed_logits)\n",
    "\n",
    "probability = custom_tensor = torch.tensor([[0.5, 2]])\n",
    "print(logits.shape, probability.shape)\n",
    "\n",
    "tensor2_expanded = probability.view(probability.size(0), probability.size(1), 1, 1)\n",
    "test = softmaxed_logits * tensor2_expanded\n",
    "\n",
    "print(test.shape)\n",
    "print(test)\n",
    "\n",
    "mean_tensor = torch.mean(test, dim=1)\n",
    "print(mean_tensor.shape)\n",
    "print(mean_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = CustomModel(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         loss = custom_loss(logits, labels)\n",
    "#         return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your model, tokenizer, and training arguments\n",
    "# model = ...  # Define your model here\n",
    "# tokenizer = ...  # Define your tokenizer here\n",
    "# training_args = TrainingArguments(\n",
    "#     ...\n",
    "# )  # Define your training arguments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = CustomTrainer(\n",
    "#     modelok=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pack to Docker Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select and Prepare a Pre-trained Seq2Seq Model\n",
    "# Generate the Response\n",
    "# Evaluation and Iteration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
