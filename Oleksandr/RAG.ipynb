{"cells":[{"cell_type":"markdown","metadata":{},"source":["PIP"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# ! wget -P ~/ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n","# ! chmod +x ~/Miniconda3-latest-Linux-x86_64.sh\n","# ! ~/Miniconda3-latest-Linux-x86_64.sh -b\n","# ! export PATH=~/miniconda3/bin:$PATH\n","# ! conda init & conda config --set auto_activate_base false\n","# # close and start a new session\n","# ! conda activate base\n","# ! conda install cudatoolkit=11.0 -y\n","# !pip install sentence-transformers   transformers datasets peft accelerate bitsandbytes faiss-cpu faiss-gpu"]},{"cell_type":"markdown","metadata":{},"source":["Imports"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n","Using device: mps\n"]},{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n","  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"]}],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, AdamW\n","from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel, PeftConfig\n","from sentence_transformers import SentenceTransformer, util\n","import faiss\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","\n","if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n","    device = torch.device(\"mps\")\n","elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")  # Fall back to CPU\n","\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{},"source":["Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    validation: Dataset({\n","        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n","        num_rows: 101093\n","    })\n","    train: Dataset({\n","        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n","        num_rows: 808731\n","    })\n","    test: Dataset({\n","        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n","        num_rows: 101092\n","    })\n","})\n"]}],"source":["dataset = load_dataset('ms_marco', 'v2.1')\n","print(dataset)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["134\n"]}],"source":["train_dataset = dataset['train'].select(range(1000))\n","train_dataset = train_dataset.filter(lambda example: example['wellFormedAnswers'] != [] and example['wellFormedAnswers'] != \"\")\n","print(len(train_dataset))"]},{"cell_type":"markdown","metadata":{},"source":["Unique Documents List"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1333\n"]}],"source":["unique_passages = set()\n","for row in train_dataset:\n","    unique_passages.update(row['passages']['passage_text'])\n","print(len(unique_passages))\n","documents = list(unique_passages)"]},{"cell_type":"markdown","metadata":{},"source":["HUgginface login"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /Users/a.diudiun/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"markdown","metadata":{},"source":["Load SentenceTransformer"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# SentenceTransformer(\"all-MiniLM-L6-v2\")\n","SentenceTranformer = SentenceTransformer(\n","    'sentence-transformers/msmarco-bert-base-dot-v5',\n","    device = device,\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["Test SentenceTransformer"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["embedding length:  768\n","Similarity: tensor([[166.5561, 159.5406]])\n"]}],"source":["query_embedding = SentenceTranformer.encode('How big is London')\n","print(\"embedding length: \", len(query_embedding))\n","document_embedding = SentenceTranformer.encode(\n","    [\n","        'London has 9,787,426 inhabitants at the 2011 census',\n","        'London is known for its finacial district',\n","    ])\n","\n","print(\"Similarity:\", util.dot_score(query_embedding, document_embedding))\n"]},{"cell_type":"markdown","metadata":{},"source":["Generate Embeddings from all documents"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Batches: 100%|██████████| 14/14 [00:13<00:00,  1.00it/s]\n"]}],"source":["# Encode documents\n","document_embeddings = SentenceTranformer.encode(\n","    documents, \n","    show_progress_bar=True, \n","    device = device,\n","    batch_size=100\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Create Faiss Index from all documents"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# faiss.normalize_L2(document_embeddings)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["index = faiss.IndexFlatIP(document_embeddings.shape[1])  # L2 distance\n","index.add(document_embeddings)"]},{"cell_type":"markdown","metadata":{},"source":["Store Faiss index to storage and read from storage"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["faiss.write_index(index, \"index_docs.index\")\n","# index = faiss.read_index(\"index_docs.index\")"]},{"cell_type":"markdown","metadata":{},"source":["Test Faiss Index"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["still alive 1\n","still alive 2\n"]}],"source":["test1_query = [\"This is a query document.\"]\n","test2_query = [\"This is a query test.\", \"This is a query donut.\"]\n","test3_query = [\"This is a glass.\", \"This is a bicycle.\", \"This is a motor.\"]\n","test1_query_embedding = SentenceTranformer.encode(test1_query)\n","test2_query_embedding = SentenceTranformer.encode(test2_query)\n","test3_query_embedding = SentenceTranformer.encode(test3_query)\n","# print(\"test_query_embedding: \", test_query_embedding)\n","print(\"still alive 1\")\n","faiss.normalize_L2(test1_query_embedding)\n","faiss.normalize_L2(test2_query_embedding)\n","faiss.normalize_L2(test3_query_embedding)\n","print(\"still alive 2\")\n","k = 5  # Number of similar documents to retrieve\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["still alive 1\n","still alive 2\n"]}],"source":["D1, I1 = index.search(test1_query_embedding, k)\n","print(\"still alive 1\")\n","D2, I2 = index.search(test2_query_embedding, k)\n","print(\"still alive 2\")\n","D3, I3 = index.search(test3_query_embedding, k)\n","print(\"still alive 3\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","print(\"still alive 33\")\n","print(\"D: \", D)\n","print(\"I: \", I)\n","\n","D_tensor = torch.tensor(D)\n","D_softmax = F.softmax(D_tensor, dim=1)  # Apply softmax along the rows\n","\n","\n","print(\"Softmax\")\n","print(D_softmax)\n","\n","D_softmax_np = D_softmax.numpy()\n","\n","print(\"Most similar documents to the query:\")\n","for i, idx in enumerate(I[0]):\n","    print(f\"Rank {i+1}: {documents[idx]}\")\n","# for i, idx in enumerate(I[1]):\n","#     print(f\"Rank {i+1}: {documents[idx]}\")"]},{"cell_type":"markdown","metadata":{},"source":["Configure LoRA and sentenceTranformer of query"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","queryTranformer = SentenceTransformer(\n","    'sentence-transformers/msmarco-bert-base-dot-v5',\n","    device = device,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Define sentenceTransformer in training mode "]},{"cell_type":"markdown","metadata":{},"source":["Define LoRA and EncoderDecoder GPT2 "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import GPT2Model, GPT2Tokenizer\n","# Generator = GPT2Model.from_pretrained('gpt2')\n","# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","generator = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n","tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side='left', pad_token = \"<|pad|>\")"]},{"cell_type":"markdown","metadata":{},"source":["Define GPT2 in training mode"]},{"cell_type":"markdown","metadata":{},"source":["Define custom lost function???\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["K = 2\n","batch_size = 2"]},{"cell_type":"markdown","metadata":{},"source":["Create training object combined????"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","# encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n","# print(train_dataset['query'])\n","\n","inputs = tokenizer(\n","    train_dataset['query'], \n","    max_length=512, \n","    padding='max_length', \n","    truncation=True,\n","    return_tensors='pt'\n",")\n","# print(type(inputs))\n","print(type(inputs['input_ids']), inputs['input_ids'].shape)\n","print(type(inputs['attention_mask']), inputs['attention_mask'].shape)\n","\n","flattened_answers = [sublist[0] for sublist in train_dataset['wellFormedAnswers']]\n","\n","targets = tokenizer(\n","    flattened_answers, \n","    max_length=512, \n","    padding='max_length', \n","    truncation=True,\n","    return_tensors='pt'\n",")\n","\n","print(type(targets['input_ids']), targets['input_ids'].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","trainingDataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], targets['input_ids'])\n","trainingDataloader = DataLoader(trainingDataset, batch_size, shuffle=False,)  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def custom_loss(logits, labels):\n","    loss = torch.mean((logits - labels) ** 2)  # For example, mean squared error\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# class QueryDataset(Dataset):\n","#     def __init__(self, inputs, targets):\n","#         self.queries = inputs\n","#         self.answers = targets\n","\n","#     def __len__(self):\n","#         return len(self.queries)\n","\n","#     def __getitem__(self, idx):\n","#         return {\n","#             'query_ids': torch.tensor(self.queries[idx], dtype=torch.float),\n","#             'answer_ids': torch.tensor(self.answers[idx], dtype=torch.float),\n","#         }\n","\n","# class GPT2Dataset(Dataset):\n","\n","#   def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n","\n","#     self.tokenizer = tokenizer\n","#     self.input_ids = []\n","#     self.attn_masks = []\n","\n","#     for txt in txt_list:\n","\n","#       encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n","\n","#       self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n","#       self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n","    \n","#   def __len__(self):\n","#     return len(self.input_ids)\n","\n","#   def __getitem__(self, idx):\n","#     return self.input_ids[idx], self.attn_masks[idx] "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SuperModel(torch.nn.Module):\n","    def __init__(self, generator, queryTranformer, index, documents):\n","        super().__init__()\n","        self.generator = generator\n","        self.queryTranformer = queryTranformer\n","        self.documents = documents\n","        self.index = index\n","\n","    def forward(self, batch_inputs_ids, attention_masks, K):\n","        \n","        queries = tokenizer.batch_decode(\n","            batch_inputs_ids,\n","            skip_special_tokens = True\n","        )\n","\n","        print(\"queries: \", queries)\n","        queries_embedding = self.queryTranformer.encode(queries)  # Pass appropriate inputs\n","        print(\"queries_embedding: \", queries_embedding)\n","        \n","\n","\n","        D, I = index.search(queries_embedding[], K)\n","\n","        print(\"D: \", D)\n","        print(\"I: \", I)\n","\n","        print(\"Most similar documents to the query:\")\n","        # for i, idx in enumerate(I[0]):\n","        #     print(f\"Rank {i+1}: {self.documents[idx]}\")\n","\n","        return\n","        D_tensor = torch.tensor(D)\n","        D_softmax = F.softmax(D_tensor, dim=1) \n","\n","        # tensor zero [Batch, K, seq_len]\n","\n","        generatorOutputs = []\n","        for i, idx in enumerate(I[0]):\n","            input_text = self.documents[idx] + \" \" + Q\n","            input_ids = tokenizer.encode(input_text, return_tensors='pt')\n","            \n","        generatorOutput = self.generator(\n","            input_ids=batch_inputs_ids\n","        )\n","\n","        generatorOutputs.append(generatorOutput)\n","\n","\n","        # run decoder on K documents and Q \n","        # avarage output from decoder \n","        return generatorOutputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = SuperModel(\n","    generator = generator , \n","    queryTranformer = queryTranformer , \n","    index = index , \n","    documents = documents ,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["optimizer = AdamW(model.parameters(), lr=5e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["i = 0\n","for batch in trainingDataloader:  # Size: Batch_size x Seq_length\n","    # input_ids, attention_mask, labels_ids = batch\n","    optimizer.zero_grad()\n","    i+=1\n","    print(len(batch), i)\n","    print(batch[0].shape, batch[1].shape, batch[2].shape)\n","    outputs = model(\n","        batch_inputs_ids = batch[0], \n","        attention_masks= batch[1], \n","        K = K\n","    )\n","    break\n","    \n","    # loss = outputs.loss\n","    # loss.backward()\n","    # optimizer.step()\n","\n","    # print(f\"Loss: {loss.item()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# class CustomTrainer(Trainer):\n","#     def compute_loss(self, model, inputs, return_outputs=False):\n","#         labels = inputs.pop(\"labels\")\n","#         outputs = CustomModel(**inputs)\n","#         logits = outputs.logits\n","#         loss = custom_loss(logits, labels)\n","#         return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Define your model, tokenizer, and training arguments\n","# model = ...  # Define your model here\n","# tokenizer = ...  # Define your tokenizer here\n","# training_args = TrainingArguments(\n","#     ...\n","# )  # Define your training arguments here"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# trainer = CustomTrainer(\n","#     modelok=model,\n","#     args=training_args,\n","#     train_dataset=train_dataset,\n","#     eval_dataset=eval_dataset,\n","#     tokenizer=tenizer,\n","# )"]},{"cell_type":"markdown","metadata":{},"source":["Run Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["Inference test data"]},{"cell_type":"markdown","metadata":{},"source":["Store model"]},{"cell_type":"markdown","metadata":{},"source":["Build API "]},{"cell_type":"markdown","metadata":{},"source":["Pack to Docker Container"]},{"cell_type":"markdown","metadata":{},"source":["Publish"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Select and Prepare a Pre-trained Seq2Seq Model\n","# Generate the Response\n","# Evaluation and Iteration\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
