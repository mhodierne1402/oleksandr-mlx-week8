{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -P ~/ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! chmod +x ~/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! ~/Miniconda3-latest-Linux-x86_64.sh -b\n",
    "# ! export PATH=~/miniconda3/bin:$PATH\n",
    "# ! conda init & conda# ! wget -P ~/ https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! chmod +x ~/Miniconda3-latest-Linux-x86_64.sh\n",
    "# ! ~/Miniconda3-latest-Linux-x86_64.sh -b\n",
    "# ! export PATH=~/miniconda3/bin:$PATH\n",
    "# ! conda init & conda config --set auto_activate_base false\n",
    "# # close and start a new session\n",
    "# ! conda activate base\n",
    "# ! conda install cudatoolkit=11.0 -y\n",
    "# ! pip install sentence-transformers   transformers datasets peft accelerate bitsandbytes \n",
    "# ! conda install -c pytorch -c nvidia faiss-gpu=1.8.0\n",
    "# !pip install faiss-gpu config --set auto_activate_base false\n",
    "# # close and start a new session\n",
    "# ! conda activate base\n",
    "# ! conda install cudatoolkit=11.0 -y\n",
    "# ! pip install sentence-transformers   transformers datasets peft accelerate bitsandbytes faiss-cpu faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, AdamW\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "if torch.backends.mps.is_available():  # Check for Apple Silicon GPU availability (requires PyTorch 1.12 or later)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Check for NVIDIA GPU availability\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fall back to CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 101093\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 808731\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
      "        num_rows: 101092\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ms_marco', 'v2.1')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].select(range(1000))\n",
    "train_dataset = train_dataset.filter(lambda example: example['wellFormedAnswers'] != [] and example['wellFormedAnswers'] != \"\")\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique Documents List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1333\n"
     ]
    }
   ],
   "source": [
    "unique_passages = set()\n",
    "for row in train_dataset:\n",
    "    unique_passages.update(row['passages']['passage_text'])\n",
    "print(len(unique_passages))\n",
    "documents = list(unique_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HUgginface login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "SentenceTranformer = SentenceTransformer(\n",
    "    'sentence-transformers/msmarco-bert-base-dot-v5',\n",
    "    device = device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding length:  768\n",
      "Similarity: tensor([[166.5561, 159.5406]])\n"
     ]
    }
   ],
   "source": [
    "query_embedding = SentenceTranformer.encode('How big is London')\n",
    "print(\"embedding length: \", len(query_embedding))\n",
    "document_embedding = SentenceTranformer.encode(\n",
    "    [\n",
    "        'London has 9,787,426 inhabitants at the 2011 census',\n",
    "        'London is known for its finacial district',\n",
    "    ])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, document_embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Embeddings from all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcf094f4d75457badd22d0b24eeaac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode documents\n",
    "document_embeddings = SentenceTranformer.encode(\n",
    "    documents, \n",
    "    show_progress_bar=True, \n",
    "    device = device,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Faiss Index from all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.normalize_L2(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(document_embeddings.shape[1])  # L2 distance\n",
    "index.add(document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Faiss index to storage and read from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"index_docs.index\")\n",
    "# index = faiss.read_index(\"index_docs.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Faiss Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = faiss.StandardGpuResources() \n",
    "# index_gpu = faiss.index_cpu_to_gpu(res, 0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = [\"This is a query test.\", \"This is a query donut.\"]\n",
    "test_query_embedding = SentenceTranformer.encode(test_query)\n",
    "# print(\"test_query_embedding: \", test_query_embedding)\n",
    "faiss.normalize_L2(test_query_embedding)\n",
    "\n",
    "k = 5  # Number of similar documents to retrieve\n",
    "D, I = index.search(test_query_embedding, k)\n",
    "print(\"D: \", D)\n",
    "print(\"I: \", I)\n",
    "D_tensor = torch.tensor(D)\n",
    "D_softmax = F.softmax(D_tensor, dim=1)  # Apply softmax along the rows\n",
    "D_softmax_np = D_softmax.numpy()\n",
    "print(D_softmax)\n",
    "print(\"Most similar documents to the query:\")\n",
    "for i, idx in enumerate(I[0]):\n",
    "    print(f\"Rank {i+1}: {documents[idx]}\")\n",
    "for i, idx in enumerate(I[1]):\n",
    "    print(f\"Rank {i+1}: {documents[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure LoRA and sentenceTranformer of query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "queryTranformer = SentenceTransformer(\n",
    "    'sentence-transformers/msmarco-bert-base-dot-v5',\n",
    "    device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sentenceTransformer in training mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LoRA and EncoderDecoder GPT2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "# Generator = GPT2Model.from_pretrained('gpt2')\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "generator = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side='left', pad_token = \"<|pad|>\")\n",
    "# 50257 - output seq\n",
    "print(generator)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define GPT2 in training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom lost function???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training object combined????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "# print(train_dataset['query'])\n",
    "max_seq_len = 768\n",
    "\n",
    "inputs = tokenizer(\n",
    "    train_dataset['query'], \n",
    "    max_length=max_seq_len, \n",
    "    padding='max_length', \n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "# print(type(inputs))\n",
    "print(type(inputs['input_ids']), inputs['input_ids'].shape)\n",
    "print(type(inputs['attention_mask']), inputs['attention_mask'].shape)\n",
    "\n",
    "flattened_answers = [sublist[0] for sublist in train_dataset['wellFormedAnswers']]\n",
    "\n",
    "targets = tokenizer(\n",
    "    flattened_answers, \n",
    "    max_length=max_seq_len, \n",
    "    padding='max_length', \n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(type(targets['input_ids']), targets['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], targets['input_ids'])\n",
    "trainingDataloader = DataLoader(trainingDataset, batch_size, shuffle=False,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(logits, labels):\n",
    "    loss = torch.mean((logits - labels) ** 2)  # For example, mean squared error\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QueryDataset(Dataset):\n",
    "#     def __init__(self, inputs, targets):\n",
    "#         self.queries = inputs\n",
    "#         self.answers = targets\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.queries)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             'query_ids': torch.tensor(self.queries[idx], dtype=torch.float),\n",
    "#             'answer_ids': torch.tensor(self.answers[idx], dtype=torch.float),\n",
    "#         }\n",
    "\n",
    "# class GPT2Dataset(Dataset):\n",
    "\n",
    "#   def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "#     self.tokenizer = tokenizer\n",
    "#     self.input_ids = []\n",
    "#     self.attn_masks = []\n",
    "\n",
    "#     for txt in txt_list:\n",
    "\n",
    "#       encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "#       self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "#       self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "#   def __len__(self):\n",
    "#     return len(self.input_ids)\n",
    "\n",
    "#   def __getitem__(self, idx):\n",
    "#     return self.input_ids[idx], self.attn_masks[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperModel(torch.nn.Module):\n",
    "    def __init__(self, generator, queryTranformer, index, documents):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.queryTranformer = queryTranformer\n",
    "        self.documents = documents\n",
    "        self.index = index\n",
    "\n",
    "    def forward(self, batch_inputs_ids, attention_masks, K, max_seq_len):\n",
    "        \n",
    "        queries = tokenizer.batch_decode(\n",
    "            batch_inputs_ids,\n",
    "            skip_special_tokens = True\n",
    "        )\n",
    "        queries_embedding = self.queryTranformer.encode(queries)  # Pass appropriate inputs\n",
    "\n",
    "        D, I = index.search(queries_embedding, K)\n",
    "\n",
    "        prompts = []\n",
    "\n",
    "        for ind in range(len(I)):\n",
    "            for i, idx in enumerate(I[ind]):\n",
    "                prompt = self.documents[idx] + \" \" + queries[ind]\n",
    "                prompts.append(prompt)\n",
    "\n",
    "        tokinized_prompts_ids = tokenizer(\n",
    "            prompts, \n",
    "            max_length=max_seq_len, \n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # print(tokinized_prompts_ids[\"input_ids\"].shape)\n",
    "        # print(tokinized_prompts_ids[\"attention_mask\"].shape)\n",
    "        # print(tokinized_prompts_ids[\"input_ids\"])\n",
    "        print(tokenizer.vocab_size)\n",
    "        print(tokenizer.pad_token)\n",
    "        generator.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        vocab_size = len(tokenizer)\n",
    "        print(\"Tokeniser vocab size: \" , vocab_size)\n",
    "\n",
    "        generator_vocab_size = generator.transformer.wte.num_embeddings\n",
    "        print(\"Generator vocab size: \", generator_vocab_size)\n",
    "\n",
    "        # if tokinized_prompts_ids[\"input_ids\"].max() >= tokenizer.vocab_size or tokinized_prompts_ids[\"input_ids\"].max() < 0:\n",
    "        #     print(\"Token out of range\")\n",
    "\n",
    "        # print(\"Generator\")\n",
    "        # print(generator)\n",
    "\n",
    "        generatorOutput = self.generator(\n",
    "            input_ids = tokinized_prompts_ids[\"input_ids\"],\n",
    "            attention_mask = tokinized_prompts_ids[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "        print(generatorOutput)\n",
    "\n",
    "        return\n",
    "        D_tensor = torch.tensor(D)\n",
    "        D_softmax = F.softmax(D_tensor, dim=1) \n",
    "\n",
    "        # tensor zero [Batch, K, seq_len]\n",
    "\n",
    "\n",
    "        # run decoder on K documents and Q \n",
    "        # avarage output from decoder \n",
    "        return generatorOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SuperModel(\n",
    "    generator = generator , \n",
    "    queryTranformer = queryTranformer , \n",
    "    index = index , \n",
    "    documents = documents ,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for batch in trainingDataloader:  # Size: Batch_size x Seq_length\n",
    "    # input_ids, attention_mask, labels_ids = batch\n",
    "    optimizer.zero_grad()\n",
    "    i+=1\n",
    "    # print(len(batch), i)\n",
    "    # print(batch[0].shape, batch[1].shape, batch[2].shape)\n",
    "    outputs = model(\n",
    "        batch_inputs_ids = batch[0], \n",
    "        attention_masks= batch[1], \n",
    "        K = K,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "    break\n",
    "    \n",
    "    # loss = outputs.loss\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    # print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = CustomModel(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         loss = custom_loss(logits, labels)\n",
    "#         return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your model, tokenizer, and training arguments\n",
    "# model = ...  # Define your model here\n",
    "# tokenizer = ...  # Define your tokenizer here\n",
    "# training_args = TrainingArguments(\n",
    "#     ...\n",
    "# )  # Define your training arguments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = CustomTrainer(\n",
    "#     modelok=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pack to Docker Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select and Prepare a Pre-trained Seq2Seq Model\n",
    "# Generate the Response\n",
    "# Evaluation and Iteration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
