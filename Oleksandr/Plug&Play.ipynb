{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00234908-384a-40d5-a897-fc1579d1342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c420e26d-3c5a-46d3-8f0a-a0d401281816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:757: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    use_auth_token = \"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\"\n",
    ")\n",
    "\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1b6e94-e6b7-46b0-bc45-06120b469576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2267339875404ba6924887d8d3b23504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \n",
    "    use_auth_token = \"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\",\n",
    "    load_in_4bit=True, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccd7390-cff8-4b7d-807c-16ef0ac54e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate the given text to Shakespearean style. I want to invite you on a second date\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    # {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "963b11bb-b7cf-43bd-b902-b34dee470183",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93c9047-5b4c-4a68-b839-cb19438e3bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:421: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Translate the given text to Shakespearean style. I want to invite you on a second date [/INST] Noble friend, I do entreat thee,\n",
      "To grant me yet another's date\n"
     ]
    }
   ],
   "source": [
    "outputs = base_model.generate(inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf1ebac-9d35-4d91-aa2c-b332cbf7703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "new_base_model = prepare_model_for_kbit_training(base_model)\n",
    "tokenizer.pad_token = \"!\"\n",
    "CUTOFF_LEN = 256\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 2 * LORA_R\n",
    "LORA_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f833f3-e0e2-4d6e-a991-5719e835a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=LORA_R, \n",
    "    lora_alpha=LORA_ALPHA, \n",
    "    target_modules=[ \"w1\", \"w2\", \"w3\"], \n",
    "    lora_dropout=LORA_DROPOUT, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "757d105d-32e6-4382-aaae-5b99c8344c35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Adapter with name shake already exists. Please use a different name.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model_with_adapter = get_peft_model(new_base_model, config)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnew_base_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshake\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:254\u001b[0m, in \u001b[0;36mPeftAdapterMixin.add_adapter\u001b[0;34m(self, adapter_config, adapter_name)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_peft_config_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m adapter_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config:\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdapter with name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madapter_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists. Please use a different name.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(adapter_config, PeftConfig):\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter_config should be an instance of PeftConfig. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(adapter_config)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Adapter with name shake already exists. Please use a different name."
     ]
    }
   ],
   "source": [
    "# model_with_adapter = get_peft_model(new_base_model, config)\n",
    "\n",
    "new_base_model.add_adapter(config, adapter_name=\"shake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f5e2a80-a800-4f8e-9584-1904fda3c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(user_query):\n",
    "  sys_msg = \"Translate the given text to Shakespearean style.\"\n",
    "  p = \"<s> [INST]\" + sys_msg +\"\\n\"+ user_query[\"modern\"] + \"[/INST]\" +  user_query[\"shakespearean\"] + \"</s>\"\n",
    "  return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b503cf06-11be-4df8-817a-f427ee9120a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['modern', 'shakespearean'],\n",
      "        num_rows: 274\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce794f3f0cf94bf489d17287602480c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"harpreetsahota/modern-to-shakesperean-translation\")\n",
    "print(\"dataset\", dataset)\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "tokenize = lambda prompt: tokenizer(prompt + tokenizer.eos_token, truncation=True, max_length=CUTOFF_LEN, padding=\"max_length\")\n",
    "train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)), remove_columns=[\"modern\" , \"shakespearean\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9093e51c-aa7b-432d-9c0f-dcfcbae0a9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "  model=new_base_model,\n",
    "  train_dataset=train_data,\n",
    "  args=TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=2,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    output_dir=\"mixtral-moe-lora-instruct-shapeskeare\"\n",
    "  ),\n",
    "  data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f02ae7cd-b232-4077-9d4b-ed377ae04e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 17:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.550700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.354500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.646800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.878100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.817800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.699300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.598500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.604200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.596900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.525600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.342100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.463900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.439200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.512800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.320000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66226de5-1b19f0c83d8db43c7e673933;1fa81b1b-ac00-4b4d-b13b-53dedf63e1d8)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\n",
      "Repo model mistralai/Mixtral-8x7B-Instruct-v0.1 is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in mistralai/Mixtral-8x7B-Instruct-v0.1.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mixtral-8x7B-Instruct-v0.1 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "new_base_model.config.use_cache = False\n",
    "trainer.train()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f23737f3-f37f-45ea-914d-9b4f23746efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer from base model:  [INST] Translate the given text to Shakespearean style. I want to invite you on a second date [/INST]I wouldst fain invite thee to a second tryst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer from new base model:  [INST] Translate the given text to Shakespearean style. I want to invite you on a second date [/INST]I wouldst fain invite thee to a second tryst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer from base model:  [INST] Translate the given text to Shakespearean style. I want to invite you on a second date [/INST]I wouldst fain invite thee to a second tryst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer from new base model:  [INST] Translate the given text to Shakespearean style. I want to invite you on a second date [/INST]I wouldst fain invite thee to a second tryst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer from base model:  [INST] Translate the given text to Shakespearean style. I want to invite you on a second date [/INST]I wouldst fain invite thee to a second tryst\n",
      "\n",
      "Answer from new base model:  [INST] Translate the given text to Shakespearean style. I want to invite you on a second date [/INST]I wouldst fain invite thee to a second tryst\n"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    outputs = base_model.generate(inputs, max_new_tokens=20)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\nAnswer from base model: \", answer)\n",
    "\n",
    "    outputs = new_base_model.generate(inputs, max_new_tokens=20)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\nAnswer from new base model: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "817cc188-bff5-4eb5-8b0f-9dc7ebde43fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "new_base_model.save_pretrained(\n",
    "    \"shake_adapter\", \n",
    "    token = \"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719662ee-24ed-4136-9849-a1e1d29769d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
